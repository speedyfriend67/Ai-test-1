<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CartPole Balancing with TensorFlow.js</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-vis"></script>
</head>
<body>
  <script>
    const model = tf.sequential({
      layers: [
        tf.layers.dense({ inputShape: [4], units: 24, activation: 'relu' }),
        tf.layers.dense({ units: 24, activation: 'relu' }),
        tf.layers.dense({ units: 2, activation: 'linear' }) // 2 actions: move left or right
      ]
    });

    model.compile({ optimizer: 'adam', loss: 'meanSquaredError' });

    const gamma = 0.95;  // Discount factor for future rewards
    const epsilon = 1.0; // Exploration-exploitation trade-off
    const epsilonDecay = 0.995;
    const epsilonMin = 0.01;
    const memory = [];

    // Function to choose an action using epsilon-greedy strategy
    function chooseAction(state) {
      if (Math.random() <= epsilon) {
        return Math.floor(Math.random() * 2); // 2 actions: 0 (left) or 1 (right)
      } else {
        const qValues = model.predict(tf.tensor2d([state])).dataSync();
        return qValues[0] > qValues[1] ? 0 : 1;
      }
    }

    // Training the model using Q-learning with experience replay
    async function trainModel() {
      const env = new rl.CartPole();
      const numEpisodes = 1000;

      for (let episode = 0; episode < numEpisodes; episode++) {
        let state = env.reset();
        let totalReward = 0;

        while (true) {
          const action = chooseAction(state);
          const nextState = env.step(action);

          // Calculate the reward
          const reward = nextState.done ? -1 : 1;

          // Store the experience in memory
          memory.push({ state, action, reward, nextState: nextState.observation, done: nextState.done });

          state = nextState.observation;
          totalReward += reward;

          if (nextState.done) {
            // Experience replay
            if (memory.length > 32) {
              const minibatch = tf.data.array(memory).shuffle(32).batch(32);
              await minibatch.forEachAsync(async (batch) => {
                const states = tf.tensor2d(batch.map((exp) => exp.state));
                const targets = model.predict(states).arraySync();

                const nextStates = tf.tensor2d(batch.map((exp) => exp.nextState));
                const nextQValues = model.predict(nextStates).max(1);

                for (let i = 0; i < batch.length; i++) {
                  targets[i][batch[i].action] = batch[i].reward + gamma * nextQValues.dataSync()[i] * (1 - batch[i].done);
                }

                await model.fit(states, tf.tensor2d(targets), { epochs: 1 });
              });
            }

            break;
          }
        }

        // Update epsilon for exploration-exploitation trade-off
        epsilon *= epsilonDecay;
        epsilon = Math.max(epsilon, epsilonMin);

        console.log(`Episode: ${episode + 1}, Total Reward: ${totalReward}`);
      }
    }

    // Start training the model
    trainModel();
  </script>
</body>
</html>
